## picture on twitter, denoted by the @ follwed by the screen name
screen_Name = c("NRC_API_Testing","CNN",
"DarrenCriss","CTVNews","MasterChef")
## object "users" automatically returned
users = lookupUsers(users= screen_Name)
# examining friendship data
friendships = lapply(users, function(x)
{
friendships(screen_names=x$screenName)
})
# One user
test_User = users[[2]]
friends = test_User$getFriends()
##  screen names can be found by looking under the profile
## picture on twitter, denoted by the @ follwed by the screen name
screen_Name = c("NRC_API_Testing","CNN",
"DarrenCriss","CTVNews","MasterChef")
## object "users" automatically returned
users = lookupUsers(users= screen_Name)
# examining friendship data
friendships = lapply(users, function(x)
{
friendships(screen_names=x$screenName)
})
# One user
test_User = users[[2]]
friends = test_User$getFriends()
friends[]
friends[1]
test_User$followRequestSent
test_User$description
test_User$lastStatus
test_User$getFavouritesCount
test_User$getFavouritesCount()
?userTimeline
userTimelin(test_User$name)
userTimeline(test_User$name)
##  screen names can be found by looking under the profile
## picture on twitter, denoted by the @ follwed by the screen name
screen_Name = c("NRC_API_Testing","CNN",
"DarrenCriss","CTVNews","MasterChef")
## object "users" automatically returned
users = lookupUsers(users= screen_Name)
# examining friendship data
friendships = lapply(users, function(x)
{
friendships(screen_names=x$screenName)
})
# One user
test_User = users[[2]]
friends = test_User$getFriends()
# test user's timeline
userTimeline(users[[3]],n=3)
USERS
users
userTimeline(users[[1]],n=3
)
##  Getting Token:
test_Token = readRDS("tokenTest")
rm(list = ls())
##  Getting Token:
test_Token = readRDS("tokenTest")
rm(list = ls())
##  libraries
library(httr)
##  Getting Token:
test_Token = readRDS("tokenTest")
rm(list = ls())
##  libraries
library(httr)
##  Getting Token (produced earlier):
test_Token = readRDS("tokenTest")
##  Making the HTTP request URL
endpoint = "https://api.twitter.com/1.1/search/tweets.json"
args = c(q = "CNN", count = 1, result_type = "mixed")
request = paste( endpoint, paste(
names(args),args,collapose = "", sep = "&"
),sep = "?")
request
rm(list = ls())
##  libraries
library(httr)
##  Getting Token (produced earlier):
test_Token = readRDS("tokenTest")
##  Making the HTTP request URL
endpoint = "https://api.twitter.com/1.1/search/tweets.json"
args = c(q = "CNN", count = "1", result_type = "mixed")
request = paste( endpoint, paste(
names(args),args,collapose = "&", sep = "="
),sep = "?")
request
paste(
names(args),args,collapose = "&", sep = "="
)
request = paste( endpoint, paste(
names(args),args,collapose = "&", sep = "="
),sep = "?",collapse = "")
request
names(args)
paste(names(args),args)
endpoint <- "https://api.flickr.com/services/rest"
arguments <- c(method="flickr.photos.search",
api_key=app_Key,
tags = "river,mountain", placeid ="",
has_geo=1,
per_page=10)
library(rvest)
rm(list = ls())
library(httr)
library(jsonlite)
library(xml2)
library(rvest)
token <- readRDS("./token.R")
app_Key <- "08adb0273ae63e5c07c249f5a621e7cb"
app_Secret <- "89e0a70bfb48e7f4"
endpoint <- "https://api.flickr.com/services/rest"
arguments <- c(method="flickr.photos.search",
api_key=app_Key,
tags = "river,mountain", placeid ="",
has_geo=1,
per_page=10)
##Finding the placeid for Canada:
place_args=c(method="flickr.places.find",
api_key = app_Key,
query="Canada")
place_Id_Request <- paste(endpoint,
paste(names(place_args), place_args,sep="=",collapse="&"),
sep="?")
place_Id_Request
rm(list = ls())
##  libraries
library(httr)
##  Getting Token (produced earlier):
test_Token = readRDS("tokenTest")
##  Making the HTTP request URL
endpoint = "https://api.twitter.com/1.1/search/tweets.json"
args = c(q = "CNN", count = "1", result_type = "mixed")
request = paste( endpoint, paste(
names(args),args,collapose = "&", sep = "="
),sep = "?",collapse = "")
args
(
names(args),args,collapose = "&", sep = "=")
rm(list = ls())
##  libraries
library(httr)
##  Getting Token (produced earlier):
test_Token = readRDS("tokenTest")
##  Making the HTTP request URL
endpoint = "https://api.twitter.com/1.1/search/tweets.json"
args = c(q = "CNN", count = "1", result_type = "mixed")
request = paste( endpoint, paste(
names(args),args,collapose = "&", sep = "="),
sep = "?",collapse = "")
request
?paste
class(args)
paste(names(args),args)
test <- paste(names(args),args)
test[1]
as.character(test)
as.character(test)[2]
rm(list = ls())
##  libraries
library(httr)
##  Getting Token (produced earlier):
test_Token = readRDS("tokenTest")
##  Making the HTTP request URL
endpoint = "https://api.twitter.com/1.1/search/tweets.json"
args = c(q = "CNN", count = "1", result_type = "mixed")
request = paste( endpoint, paste(names(args),args,collapose = "&", sep = "="),
sep = "?",collapse = "")
request
v1 <- c(1,2,3)
v2 <- c(4,5,6)
paste(v1,v2)
rm(list = ls())
##  libraries
library(httr)
##  Getting Token (produced earlier):
test_Token = readRDS("tokenTest")
##  Making the HTTP request URL
endpoint = "https://api.twitter.com/1.1/search/tweets.json"
args = c(q = "CNN", count = "1", result_type = "mixed")
request = paste( endpoint, paste(names(args),args,collapse = "&", sep = "="),
sep = "?",collapse = "")
request
?GET
raw <- GET(requst, config=c(token=test_Token))
raw <- GET(request, config=c(token=test_Token))
raw
rm(list = ls())
##  libraries
library(httr)
##  Getting Token (produced earlier):
test_Token = readRDS("tokenTest")
##  Making the HTTP request URL
endpoint = "https://api.twitter.com/1.1/search/tweets.json"
args = c(q = "CNN", count = "1", result_type = "mixed")
request = paste( endpoint, paste(names(args),args,collapse = "&", sep = "="),
sep = "?",collapse = "")
## Getting raw Response
raw <- GET(request, config=(token=test_Token))
raw
library(httr)
library(jsonlite)
response <-fromJSON(rawToChar(raw$content))
response
names(response)
response$statuses$entities
response$statuses$metadata
response$search_metadata
if(400!= 200){
warnings("Request not successful")
}
?warnings
?write
?warning
if(400!= 200){
warning("Request not successful")
}
rm(list = ls())
##  libraries
library(httr)
library(jsonlite)
##  Getting Token (produced earlier):
test_Token = readRDS("tokenTest")
##  Making the HTTP request URL
endpoint = "https://api.twitter.com/1.1/search/tweets.json"
args = c(q = "CNN", count = "1", result_type = "mixed")
request = paste( endpoint, paste(names(args),args,collapse = "&", sep = "="),
sep = "?",collapse = "")
## Getting raw Response
raw <- GET(request, config=(token=test_Token))
if(raw$status_code!= 200){
warning("Request not successful")
}
response <-fromJSON(rawToChar(raw$content))
names(response)
##metadata for tweet
meta <- response$statuses$metadata
## metadata for search query
query_Meta <- response$search_metadata
## Entities information
entities <- response$statuses$entities
entities
names(entities)
entities$urls
url <- entities$urls
class(url)
url
url <- entities$urls["expanded_url"]
url
entities$urls
names(url)
url <- entities$urls
url <- entities$urls
url
names(url)
unlist(url)
url <- unlist(entities$urls)
url["url"]
rm(list = ls())
##  libraries
library(httr)
library(jsonlite)
##  Getting Token (produced earlier):
test_Token <- readRDS("tokenTest")
##  Making the HTTP request URL
endpoint <- "https://api.twitter.com/1.1/search/tweets.json"
args <- c(q = "CNN", count = "1", result_type = "mixed")
request <- paste( endpoint, paste(names(args),args,collapse = "&", sep = "="),
sep <- "?",collapse = "")
## Getting raw Response
raw <- GET(request, config=(token=test_Token))
if(raw$status_code!= 200){
warning("Request not successful")
}
response <-fromJSON(rawToChar(raw$content))
raw$status_code
request
rm(list = ls())
##  libraries
library(httr)
library(jsonlite)
##  Getting Token (produced earlier):
test_Token <- readRDS("tokenTest")
##  Making the HTTP request URL
endpoint <- "https://api.twitter.com/1.1/search/tweets.json"
args <- c(q = "CNN", count = "1", result_type = "mixed")
request <- paste( endpoint, paste(names(args),args,collapse = "&", sep = "="),
sep = "?",collapse = "")
## Getting raw Response
raw <- GET(request, config=(token=test_Token))
if(raw$status_code!= 200){
warning("Request not successful")
}
response <-fromJSON(rawToChar(raw$content))
names(response)
##metadata for tweet
meta <- response$statuses$metadata
## metadata for search query
query_Meta <- response$search_metadata
## Entities information
entities <- response$statuses$entities
## Expanded URL:
url <- unlist(entities$urls)
expanded_Url <- url["expanded_url"]
rm(list = ls())
##  libraries
library(httr)
library(jsonlite)
##  Getting Token (produced earlier):
test_Token <- readRDS("tokenTest")
##  Making the HTTP request URL
endpoint <- "https://api.twitter.com/1.1/search/tweets.json"
args <- c(q = "CNN", count = "1", result_type = "mixed")
request <- paste( endpoint, paste(names(args),args,collapse = "&", sep = "="),
sep = "?",collapse = "")
## Getting raw Response
raw <- GET(request, config=(token=test_Token))
if(raw$status_code!= 200){
warning("Request not successful")
}
response <-fromJSON(rawToChar(raw$content))
names(response)
##metadata for tweet
meta <- response$statuses$metadata
## metadata for search query
query_Meta <- response$search_metadata
## Entities information
entities <- response$statuses$entities
## Expanded URL:
url <- unlist(entities$urls)
expanded_Url <- url["expanded_url"]
expanded_Url
rm(list = ls())
raw_File = "temperature - Copy.csv"
raw_Data = read.table(file=raw_File,header=TRUE,
sep=",",stringsAsFactors = FALSE)
length=dim(raw_Data)[1]
raw_Data = raw_Data[seq(from=1,to=length,by=2),]
## Lagging each column, starting from 3rd column
## made into a quick function.
LagData = function(data,start_Col=3, lag=1, lag_Increment=1){
for( i in start_Col: length(names(data))){
data[,start_Col] = data.table::shift(
x= data[,start_Col], n=lag
)
start_Col = start_Col+1
lag =lag+lag_Increment
}
return(data)
}
lagged = LagData(data=raw_Data, lag_Increment = 1)
##  Selecting only the complete data
library(dplyr)
data = lagged %>%
filter(!is.na(forcast.temperature.in.24h..C.))
##  Changing the column names:
col_Names = c("Date","current")
for(i in 3:length(names(data))){
col_Names = append(col_Names, paste("For",i-2,"h",sep=""))
}
colnames(data) <- col_Names
rm(list = ls())
raw_File = "temperature - Copy.csv"
raw_Data = read.table(file=raw_File,header=TRUE,
sep=",",stringsAsFactors = FALSE)
length=dim(raw_Data)[1]
raw_Data = raw_Data[seq(from=1,to=length,by=2),]
## Lagging each column, starting from 3rd column
## made into a quick function.
LagData = function(data,start_Col=3, lag=1, lag_Increment=1){
for( i in start_Col: length(names(data))){
data[,start_Col] = data.table::shift(
x= data[,start_Col], n=lag
)
start_Col = start_Col+1
lag =lag+lag_Increment
}
return(data)
}
lagged = LagData(data=raw_Data, lag_Increment = 1)
##  Selecting only the complete data
library(dplyr)
data = lagged %>%
filter(!is.na(forcast.temperature.in.24h..C.))
##  Changing the column names:
col_Names = c("Date","current")
for(i in 3:length(names(data))){
col_Names = append(col_Names, paste("For",i-2,"h",sep=""))
}
colnames(data) <- col_Names
rm(list = setdiff(ls(),"data"))
##  include: the list of cols(names/index) to include for
##  data frame
##  Returns new data frame containing
##  date, current temp,closest worse perdication hours wanted,max and min error bars
Organize = function(data, include){
new_Data =data[include]
##  Max and Min predictions
max = apply(new_Data,1,function(x){ max(x)} )
min = apply(new_Data,1,function(x){ min(x)} )
##  Processing the index(hour) of maximum difference
max_Location = vector()
for(i in 1: length(max)){
row = new_Data[i,]
##  Which number to look for
if(abs(max[i] - data$current[i])>
abs(min[i] - data$current[i]))
locate = max[i]
else
locate = min[i]
##  Finding index(forecast hour)
if(max[i]==data$current[i] &
min[i]==data$current[i])
max_Location[i]=0
else
max_Location[i]= match(locate, row)
}
##  attaching to a data frame.
new_Data = cbind(date = data$Date,
current = data$current,
new_Data,
maxPredict=max,
minPredict=min,
max_Diff = max_Location)
##Adding the erMax and erMin from current Temp
new_Data = new_Data %>%
mutate(erMax=maxPredict-current)%>%
mutate(erMin=minPredict-current)
return(new_Data)
}
names = colnames(data)
include24h = grep("For1h",names):
grep("For24h",names)
data24h = Organize(data,include24h )
include12h = grep("For1h",names):
grep("For12h",names)
data12h = Organize(data, include12h)
include6h = grep("For1h",names):
grep("For6h",names)
data6h = Organize(data, include6h)
plotTimeTempSeries = function(data, title){
library(ggplot2)
plot = ggplot(data=data, aes(x=strptime(data$date,format="%m/%d/%Y %H:%M"),
y=data$current) ) + geom_point(color="red",size=2.5)+
scale_x_datetime() +
##  titles and things
xlab("Time")+ylab("Temp in C")+
ggtitle(title)+
##  Error Bar
geom_errorbar(aes(ymin=data$current,
ymax = data$maxPredict),
color = "blue")+
geom_errorbar(aes(ymin=data$minPredict,
ymax = data$current),
color="blue")
}
library(ggplot2)
library(gridExtra)
p24h = plotTimeTempSeries(data24h,"Time vs Temp for 24h forecast")
p12h = plotTimeTempSeries(data12h,"Time vs Temp for 12h forecast")
p6h = plotTimeTempSeries(data6h,"Time vs Temp for 6h forecast")
grid.arrange(p24h,p6h,p12h)
##  Save the graphs for better viewing
ggsave("./Pictures/p24h.png", p24h, device="png")
ggsave("./Pictures/p12h.png", p12h, device="png")
ggsave("./Pictures/p6h.png", p6h, device="png")
plotEr = function(data, title){
library(ggplot2)
library(tidyr)
library(dplyr)
## joining the erUp and erDown
plot_Data = select(data, c(date,erMax,erMin))
plot_Data = gather(plot_Data, key="Type",
value = "value",erMax,erMin)
plot = ggplot(data=plot_Data, aes(x=strptime(plot_Data$date,format="%m/%d/%Y %H:%M"),
y=value, color=Type) ) + geom_line()+
scale_x_datetime() +
##  titles and things
xlab("Time")+ylab("Magnitude of Error")+
ggtitle(title)+facet_wrap(~Type)
return(plot)
}
p24hEr = plotEr(data24h,"Er vs Time")
p24hEr
ggsave(file="./Pictures/p24hEr.png",p24hEr,device="png")
plotWorstAsTime = function(data, title){
library(ggplot2)
plot = ggplot(data=data, aes(x=strptime(data$date,format="%m/%d/%Y %H:%M"),
y=max_Diff) ) + geom_point()+
scale_x_datetime() +
##  titles and things
xlab("Time")+ylab("Earliest Worst forecast Hour")+
ggtitle(title)
return(plot)
}
forWT = plotWorstAsTime(data24h,"Earliest Worst Prediction")
##  Histograms
forWTH = ggplot(data = as.data.frame(
table(data24h$max_Diff)),
aes(x=Var1,y=Freq))+
geom_bar(stat="identity")+
xlab("forecast Hour")+ ylab("Frequency")+
ggtitle("Frequency of Worst forecast Hours")
ggsave("./Pictures/ClosestWorstPredication.png", forWTH, device="png")
write.table(data24h,file="24hForecast.csv",sep=",",
col.names = TRUE, row.names=FALSE)
forWTH
names(data24h)
