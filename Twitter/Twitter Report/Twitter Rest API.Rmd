---
title: "Twitter Rest API"
author: "paul"
date: "March 14, 2017"
output: 
    html_document:
        toc: true
        toc.depth: 6
---

```{r Rest API setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


*****

### Tools and Packages:
*   [Apigee Twitter API Console](https://apigee.com/console/twitter)
    +   Useful interface to test out queries to the API.
*   [twitteR](https://cran.r-project.org/web/packages/twitteR/twitteR.pdf)
*   [twitteR Vignette](http://geoffjentry.hexdump.org/twitteR.pdf)
    +   __Highly Recommanded to Read__


*****

### First Look at _twitteR_:
*   The package provides methods for parsing returned data.
*   Methods to setup, store, load twitter data bases.
*   Classes/objects wrappersto represent twitter objects. 
    +   includes functions to convert to Data frames.
*   Extracting personal data:
    +   favorites
    +   friendships
    +   direct messages
*   Public data:
    +   trends
    +   public tweets
    +   retweets
    +   search Twitter
*   Create access tokens 
    (handling handshake between brower and Twitter Server)
*   Many customizable parameters. 
    +   Abstracts away from retrying in case of rate limit
    +   Abstracts away from manual navigation of cursors
    
*   Uncertain as to how to access the _.httr-oauth_
*   



### Tweeter Limits And Information:
*   Can extract up to a maximum 3200 statuses from a user Timeline.
    +   Each page of response can contain up to 200 results. 
*   For [search/tweets](https://dev.twitter.com/rest/reference/get/search/tweets), each page of response
    can contain up to 100 tweets.
*   The _source owner_ is mentioned in the text of the tweet (status in twitteR package) by an _@_ sign followed
    by the source owner of the tweet. 
    
    
    


***** 


### Example 1: Creating a Word Cloud From Twitter 

__GOAL: To test out the _twitteR_ package's ability to
    scrape public twitter data.__

*   [Google Building wordCloud](https://sites.google.com/site/miningtwitter/questions/talking-about/wordclouds/wordcloud1)
*   [ Building wordCloud](https://www.r-bloggers.com/building-wordclouds-in-r/)
*   [Using tm Package](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf)
    
1:  Import Libraries and getting Access Token
```{r AAA Example1: libraries + Token, results = "hide",error=T,warning=FALSE}
    
    rm(list = ls())

    library(twitteR)
    library(httr)

    library(tm)
    library(wordcloud)
    library(SnowballC)
    library(RColorBrewer)

    # access Tokens
    consumer_Key = "oQ3PqERg75kPtgBcgOLaFShSC"
    consumer_Secret = "d4cxaKc1Dt3ugagruUNPtWzvmqGHx8WvwYAQ8MywUqTIVTTj9O"
    access_Token = "833674399224061952-tL4gGOyGUrz84IbVlkkAmQzqUPahL1N"
    access_Secret = "qkNmkD7TU5uZtIENW3r5K20wkqfbL6w37xyXLwelYBZg6"
    
    
    ##  Intially, the function asks the user to cache the credentials and 
    ##  will be used for another session.
    setup_twitter_oauth(consumer_Key,consumer_Secret,access_Token,access_Secret)
    
    
    
    ##  Creating a token
    app <- oauth_app("twitter", key=consumer_Key, secret=consumer_Secret)

    token = Token1.0$new(endpoint = NULL, params = list(as_header = TRUE),
                                app = app, credentials = list(oauth_token = access_Token,
                      oauth_token_secret = access_Secret))
    saveRDS(token,"tokenTest")
    test_Token = readRDS("tokenTest")
    
    ## token and test_Token are the same object.
    
    ## loading cached token from twitteR package:
    ##  It failed, likely because of the output format.
    ##  The file size is 0KB
   # oauth_content <- readRDS('.httr-oauth')

    
```

2:  Getting Raw Data

```{r AAA Example1: Getting Raw Data}

    search_String = "NRC+OR+#NRC+OR+@NRC"
    lang = "en"
    since = "2016-01-01"
   
    
    ##  Extracting coordinates for center of Canada:
    google_Api_Key = "AIzaSyBGTs-gZCbyP8n0Hvw_VZ76Z6YrST1DNa8"
    google_Host = "https://maps.googleapis.com/maps/api"
    
    request = paste(google_Host,"/geocode/json",
                    "?address=Canada&key=",
                    google_Api_Key,sep="")
    raw= GET(request)
    
    data = jsonlite::fromJSON(
       httr:: content(raw,as="text")
        )
    lat = data$results$geometry$location["lat"]
    lng = data$results$geometry$location["lng"]
    
    geocode =paste(lat,lng,"2000km",sep=",")
    
    
     
    ##  A list of tweets in Ottawa mentioning NRC. Note, the return 
    ## already a "status"
    NRC_Search = searchTwitter(search_String, n=200, 
                               lang=lang, since=since,geocode =geocode)
    
    ##  Does allow the specification of "untruncated tweets"
    ##  This was done manually.
    
    
    ##  Many tweets are truncated. Getting a list
    ## of ids for tweets that have been truncated.
    
    truncated_Id = lapply(NRC_Search, function(x)
            {
                if(x$truncated)
                   return(x$id)
                else
                    return(NA)
            }
        )
    
    version = 1.1
    cmd = "/statuses/show/"
    param = "?tweet_mode=extended"
    
    search_Id = truncated_Id[
        !is.na(truncated_Id)]
    
    long_Tweet = list();
    
    ##  Getting Untruncated tweets
    ##  Going to use traditionall get methods
    for(i in 1:length(search_Id))
    {
        url = paste("https://api.twitter.com/",
                    version,cmd,
                    search_Id[i],".json",
                    param, sep="")
        
        ##getting raw response
        raw_Response = GET(url,config=token)
        
        ##  expanded
        long_Tweet[[i]] = jsonlite::fromJSON(
            httr::content(raw_Response,"text")
        )
        
    }
    
    
    truncated_Id[!is.na(truncated_Id)] = long_Tweet
    
    ## Organized texts results
    for(i in 1:length(NRC_Search)){
        if(NRC_Search[[i]]$truncated){
            NRC_Search[[i]] = truncated_Id[[i]]

        }
    }
    
    
    ## removing twitter links
    text_NRC = lapply(NRC_Search, function(x)
            {
                text = x$text
                ## converting to ASCII
                text= iconv(x=text,from="UTF-8",to="ASCII",sub="")
                
                ##Clearning out URLs
                text=gsub("http(s?)://t.co/[a-zA-Z0-9]+", 
                     "",text)
                text=gsub("\nhttps:","",text)
            }
        )
    
    
    
     rm(list = setdiff(ls(),c("token","text_NRC")))
    
    
    
      rate_Limit = getCurRateLimitInfo()
```


3:  Make the word Map
```{r AAA Example1: Making Word Map}
    
    ##  Constructing corpus (structure to process text)
    cor= Corpus(VectorSource(text_NRC))
    cor = tm_map(cor,removePunctuation)
    cor = tm_map(cor, removeWords,stopwords('english'))
    cor = tm_map(cor,stemDocument)
    
    
    
    ## Some words may appear to be missing characters
    ## This is due to the stem analysis function.
    
    wordcloud(cor,max.words = 100,random.color=T,random.order = T,
              colors =brewer.pal(8,"Paired") )
```

```{r, include=FALSE}
```


*****

### REST API Example 2: Random Exploration 

```{r REST Example2:}

    ## Useful for examining rate 
    ## remaining in 15 window
    rate_Limit = getCurRateLimitInfo()

```


1:  Friendships and Users

```{r REST Example2: Users And Information}
    ## Screen names have no spaces in betwen

    screen_Name = c("NRC_API_Testing","CNN",
        "DarrenCriss","CTVNews","MasterChef")

    ## object "users" automatically returned
    users = lookupUsers(users= screen_Name)

    # examining friendship data
    friendships = lapply(users, function(x)
    {
        friendships(screen_names=x$screenName)
    })
    
    # One user
    test_User = users[[2]]
    
    friends = test_User$getFriends()
    
```

2:  Favorites:

```{r REST Example2: Favorites}

    
    username="NRC_API_Testing"
    
    ##  first 50 favorite posts, 
    ## only 1 post was favorited, only returned 1 element,
    ## instead of a list of NULLS or NAs for the remaining
    ## elements.
    my_Favorites = favorites(user=username, n=50)
    
```

3: Friendships(Could be used for social graphs) only for the Owner:

```{r REST Example2: Friendships}

    names = list("NRC_API_Testing","MASTERCHEFonFOX",
              "DarrenCriss","LastWeekTonight","NRC_CNRC",
              "CNN")

    my_Friendships = friendships(names)

```


4:  Trending Section of Twitter:
*   the trending section describes the popular
    live disscussions. 
```{r REST Example2: Trending}

    google_Api_Key = "AIzaSyBGTs-gZCbyP8n0Hvw_VZ76Z6YrST1DNa8"
    google_Host = "https://maps.googleapis.com/maps/api"
    
    ##  A list of available trend locations
    locations = availableTrendLocations()
    
    ##  Closest trending location
    request = paste(google_Host,"/geocode/json",
                    "?address=Ottawa,Canada&key=",
                    google_Api_Key,sep="")
    raw= GET(request)
    
    data = jsonlite::fromJSON(
       httr:: content(raw,as="text")
        )
    ottawa_Lat = data$results$geometry$location["lat"]
    ottawa_Lng = data$results$geometry$location["lng"]
    closest_Location = closestTrendLocations(lat=ottawa_Lat,
                                             long=ottawa_Lng)
    
    
    ##  The following returns names and search URLs
    trend = getTrends(closest_Location$woeid)
  
    ##  Using the Search API to retrieve information
    search_Data = searchTwitter(searchString=trend$query[[2]],n=10,
                                lang="en")
    
  

```



### Example 3: DataBase Connection Functions of twitteR:

```{r Example 3: Clearing Envrionment}

    rm(list=setdiff(ls(),c("token","search_Data")))
    library(twitteR)

```



```{r Example 3: Saving data}

    db_Data = twitteR::twListToDF(search_Data)
    
      ##sorting text in search_Data
    for(i in 1: length(db_Data$text)){
        db_Data[[i]] = gsub(pattern="\n", replacement = " ",
                            db_Data[[i]])
    }


    if(!file.exists("DB_Data.txt"))
    {
        file.create("DB_Data.txt")
       
    }

     write.table(db_Data, file="./DB_Data.txt",row.names = F,
                fileEncoding = "UTF-8",sep="\t",col.names=F)

```


### REST Example 3: Testing Provided Data Base Functions:
__GOAL: Teseting the functionality of connecting to data bases.__
*   19970728Paul$

*   A very simple database containing the 10 tweets from the _db_Data_ 
    and stored it in a local sql data base. 

```{r REST Example 3: MySqlData}
require(RMySQL)
require(twitteR)


    db_name = "twitterdb"
    user = "root"
    host ="localhost"
    password = "19970728Paul$"
    
    ## sets up a connection
    DBI = register_mysql_backend(db_name,host,user,password)

    ## returns a list of twitteR status
    loaded_Data = load_tweets_db(table_name = "status")
    
    ## Trying to store tweets into the same db:
      search_Data2 = searchTwitter(searchString="#glee",n=10,
                                lang="en")
      
    ## The new data is appended to the bottom 
    store_tweets_db(search_Data2,table_name="status")
    
    
```

### REST API Example 4: Tweets maximum:
__GOAL: Test to see how many tweets can twitter return.__
*   The maximum number of tweets is 3200. 
    
```{r REST API Example 4: Maximum, include = false }
    rm(list = setdiff(ls(),c("token")))
    require(twitteR)

    ## Creation date of Twitter
    max = 2000
    not_Max = TRUE
    
    while(not_Max == TRUE)
    {
         test = searchTwitter(searchString="CNN",n=max)
         
         if(length(test) < max)
         {
             not_Max == FALSE;
             max = length(test)
         }
             
         else
             max= max+200;
         
         print(Sys.time())
             
    }
   
    
    
    dates =lapply(time_Test,function(x){
                x$created
            }
        )


```




### Evaluation of _twitteR_:
*   The built in glass wrappers provides
    convient and organized information. 
    +   functions associated with these class
        are very useful. 
*   The built in function provides substantial 
    details and is shown in an easily accessible
    way.
*   Unlike facebook, many users are public and thus
    the account details are easily accessed. 