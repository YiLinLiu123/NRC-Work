library(SnowballC)
library(RColorBrewer)
# access Tokens
consumer_Key = "oQ3PqERg75kPtgBcgOLaFShSC"
consumer_Secret = "d4cxaKc1Dt3ugagruUNPtWzvmqGHx8WvwYAQ8MywUqTIVTTj9O"
access_Token = "833674399224061952-tL4gGOyGUrz84IbVlkkAmQzqUPahL1N"
access_Secret = "qkNmkD7TU5uZtIENW3r5K20wkqfbL6w37xyXLwelYBZg6"
##  Intially, the function asks the user to cache the credentials and
##  will be used for another session.
setup_twitter_oauth(consumer_Key,consumer_Secret,access_Token,access_Secret)
##  Creating a token
app <- oauth_app("twitter", key=consumer_Key, secret=consumer_Secret)
token = Token1.0$new(endpoint = NULL, params = list(as_header = TRUE),
app = app, credentials = list(oauth_token = access_Token,
oauth_token_secret = access_Secret))
saveRDS(token,"tokenTest")
test_Token = readRDS("tokenTest")
## token and test_Token are the same object.
## loading cached token from twitteR package:
##  It failed, likely because of the output format.
##  The file size is 0KB
# oauth_content <- readRDS('.httr-oauth')
search_String = "NRC+OR+#NRC+OR+@NRC"
lang = "en"
since = "2016-01-01"
##  Extracting coordinates for center of Canada:
google_Api_Key = "AIzaSyBGTs-gZCbyP8n0Hvw_VZ76Z6YrST1DNa8"
google_Host = "https://maps.googleapis.com/maps/api"
request = paste(google_Host,"/geocode/json",
"?address=Canada&key=",
google_Api_Key,sep="")
raw= GET(request)
data = jsonlite::fromJSON(
httr:: content(raw,as="text")
)
lat = data$results$geometry$location["lat"]
lng = data$results$geometry$location["lng"]
geocode =paste(lat,lng,"2000km",sep=",")
##  A list of tweets in Ottawa mentioning NRC. Note, the return
## already a "status"
NRC_Search = searchTwitter(search_String, n=200,
lang=lang, since=since,geocode =geocode)
##  Does allow the specification of "untruncated tweets"
##  This was done manually.
##  Many tweets are truncated. Getting a list
## of ids for tweets that have been truncated.
truncated_Id = lapply(NRC_Search, function(x)
{
if(x$truncated)
return(x$id)
else
return(NA)
}
)
version = 1.1
cmd = "/statuses/show/"
param = "?tweet_mode=extended"
search_Id = truncated_Id[
!is.na(truncated_Id)]
long_Tweet = list();
##  Getting Untruncated tweets
##  Going to use traditionall get methods
for(i in 1:length(search_Id))
{
url = paste("https://api.twitter.com/",
version,cmd,
search_Id[i],".json",
param, sep="")
##getting raw response
raw_Response = GET(url,config=token)
##  expanded
long_Tweet[[i]] = jsonlite::fromJSON(
httr::content(raw_Response,"text")
)
}
truncated_Id[!is.na(truncated_Id)] = long_Tweet
## Organized texts results
for(i in 1:length(NRC_Search)){
if(NRC_Search[[i]]$truncated){
NRC_Search[[i]] = truncated_Id[[i]]
}
}
## removing twitter links
text_NRC = lapply(NRC_Search, function(x)
{
text = x$text
## converting to ASCII
text= iconv(x=text,from="UTF-8",to="ASCII",sub="")
##Clearning out URLs
text=gsub("http(s?)://t.co/[a-zA-Z0-9]+",
"",text)
text=gsub("\nhttps:","",text)
}
)
rm(list = setdiff(ls(),c("token","text_NRC")))
rate_Limit = getCurRateLimitInfo()
##  Constructing corpus (structure to process text)
cor= Corpus(VectorSource(text_NRC))
cor = tm_map(cor,removePunctuation)
cor = tm_map(cor, removeWords,stopwords('english'))
cor = tm_map(cor,stemDocument)
## Some words may appear to be missing characters
## This is due to the stem analysis function.
wordcloud(cor,max.words = 100,random.color=T,random.order = T,
colors =brewer.pal(8,"Paired") )
## Useful for examining rate
## remaining in 15 window
rate_Limit = getCurRateLimitInfo()
## Screen names have no spaces in betwen
screen_Name = c("NRC_API_Testing","CNN",
"DarrenCriss","CTVNews","MasterChef")
## object "users" automatically returned
users = lookupUsers(users= screen_Name)
# examining friendship data
friendships = lapply(users, function(x)
{
friendships(screen_names=x$screenName)
})
username="NRC_API_Testing"
##  first 50 favorite posts,
## only 1 post was favorited, only returned 1 element,
## instead of a list of NULLS or NAs for the remaining
## elements.
my_Favorites = favorites(user=username, n=50)
names = list("NRC_API_Testing","MASTERCHEFonFOX",
"DarrenCriss","LastWeekTonight","NRC_CNRC",
"CNN")
my_Friendships = friendships(names)
google_Api_Key = "AIzaSyBGTs-gZCbyP8n0Hvw_VZ76Z6YrST1DNa8"
google_Host = "https://maps.googleapis.com/maps/api"
##  A list of available trend locations
locations = availableTrendLocations()
##  Closest trending location
request = paste(google_Host,"/geocode/json",
"?address=Ottawa,Canada&key=",
google_Api_Key,sep="")
raw= GET(request)
data = jsonlite::fromJSON(
httr:: content(raw,as="text")
)
ottawa_Lat = data$results$geometry$location["lat"]
ottawa_Lng = data$results$geometry$location["lng"]
closest_Location = closestTrendLocations(lat=ottawa_Lat,
long=ottawa_Lng)
##  The following returns names and search URLs
trend = getTrends(closest_Location$woeid)
##  Using the Search API to retrieve information
search_Data = searchTwitter(searchString=trend$query[[2]],n=10,
lang="en")
rm(list=setdiff(ls(),c("token","search_Data")))
library(twitteR)
db_Data = twitteR::twListToDF(search_Data)
if(!file.exists("DB_Data.csv"))
{
file.create("DB_Data.csv")
}
write.table(db_Data, file="./DB_Data.csv",row.names = F,
fileEncoding = "UTF-8",sep=",")
names(search_Data)
head(db_Data,n=1)
db_Data = twitteR::twListToDF(search_Data)
if(!file.exists("DB_Data.csv"))
{
file.create("DB_Data.csv")
}
write.table(db_Data, file="./DB_Data.csv",row.names = F,
fileEncoding = "UTF-8",sep=",")
db_Data = twitteR::twListToDF(search_Data)
if(!file.exists("DB_Data.txt"))
{
file.create("DB_Data.txt")
}
write.table(db_Data, file="./DB_Data.csv",row.names = F,
fileEncoding = "UTF-8",sep="\t")
db_Data = twitteR::twListToDF(search_Data)
if(!file.exists("DB_Data.txt"))
{
file.create("DB_Data.txt")
}
write.table(db_Data, file="./DB_Data.txt",row.names = F,
fileEncoding = "UTF-8",sep="\t")
?write.table
db_Data = twitteR::twListToDF(search_Data)
if(!file.exists("DB_Data.txt"))
{
file.create("DB_Data.txt")
}
write.table(db_Data, file="./DB_Data.txt",row.names = F,
fileEncoding = "UTF-8",sep="\t",row.names=F)
db_Data = twitteR::twListToDF(search_Data)
if(!file.exists("DB_Data.txt"))
{
file.create("DB_Data.txt")
}
write.table(db_Data, file="./DB_Data.txt",row.names = F,
fileEncoding = "UTF-8",sep="\t",col.names=F)
for(text in db_Data$text){
print(text)
}
?gsub
db_Data = twitteR::twListToDF(search_Data)
##sorting text in search_Data
for(i in 1: length(db_Data$text)){
db_Data[[i]] = gsub(pattern="\n", replacement = " ",
db_Data[[i]])
}
if(!file.exists("DB_Data.txt"))
{
file.create("DB_Data.txt")
}
write.table(db_Data, file="./DB_Data.txt",row.names = F,
fileEncoding = "UTF-8",sep="\t",col.names=F)
?register_mysql_backend
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(streamR)
library(ROAuth)
library(twitteR)
library(httr)
require(streamR)
require(ROAuth)
require(twitteR)
require(httr)
## Taken from examples in streamR Package Documentation
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "http://api.twitter.com/oauth/access_token"
authURL <- "http://api.twitter.com/oauth/authorize"
consumer_Key = "oQ3PqERg75kPtgBcgOLaFShSC"
consumer_Secret = "d4cxaKc1Dt3ugagruUNPtWzvmqGHx8WvwYAQ8MywUqTIVTTj9O"
my_oauth <- OAuthFactory$new(consumerKey=consumer_Key,
consumerSecret=consumer_Secret,
requestURL=requestURL,accessURL=accessURL, authURL=authURL)
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
require(streamR)
require(ROAuth)
require(twitteR)
require(httr)
## Taken from examples in streamR Package Documentation
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "http://api.twitter.com/oauth/access_token"
authURL <- "http://api.twitter.com/oauth/authorize"
consumer_Key = "oQ3PqERg75kPtgBcgOLaFShSC"
consumer_Secret = "d4cxaKc1Dt3ugagruUNPtWzvmqGHx8WvwYAQ8MywUqTIVTTj9O"
my_oauth <- OAuthFactory$new(consumerKey=consumer_Key,
consumerSecret=consumer_Secret,
requestURL=requestURL,accessURL=accessURL, authURL=authURL)
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
require(streamR)
require(ROAuth)
require(twitteR)
require(httr)
## Taken from examples in streamR Package Documentation
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "http://api.twitter.com/oauth/access_token"
authURL <- "http://api.twitter.com/oauth/authorize"
consumer_Key = "oQ3PqERg75kPtgBcgOLaFShSC"
consumer_Secret = "d4cxaKc1Dt3ugagruUNPtWzvmqGHx8WvwYAQ8MywUqTIVTTj9O"
my_oauth <- OAuthFactory$new(consumerKey=consumer_Key,
consumerSecret=consumer_Secret,
requestURL=requestURL,accessURL=accessURL, authURL=authURL)
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
require(streamR)
require(ROAuth)
require(twitteR)
require(httr)
## Taken from examples in streamR Package Documentation
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "http://api.twitter.com/oauth/access_token"
authURL <- "http://api.twitter.com/oauth/authorize"
consumer_Key = "oQ3PqERg75kPtgBcgOLaFShSC"
consumer_Secret = "d4cxaKc1Dt3ugagruUNPtWzvmqGHx8WvwYAQ8MywUqTIVTTj9O"
my_oauth <- OAuthFactory$new(consumerKey=consumer_Key,
consumerSecret=consumer_Secret,
requestURL=requestURL,accessURL=accessURL, authURL=authURL)
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
rm(list=ls())
install.packages("exif")
devtools::install_github("ironholds/exif")
install.packages("devtools")
library(devtools)
devtools::install_github("ironholds/exif")
install.packages("EXIF")
install.packages("EXIF")
devtools::install_github("ironholds/exif")
devtools::install_github("ironholds/exif")
devtools::install_github("ironholds/exif")
install.packages("Rcpp")
install.packages("Rcpp")
devtools::install_github("ironholds/exif")
library(exif)
?read_exif
data= read_exif("./Test Pictures/tiger.jpg")
data= read_exif("./Test Pictures/lake.jpg")
data
animal= read_exif("./Test Pictures/animal1.jpg")
animal
animal= read_exif("./Test Pictures/animal1.jpg")
lake = read_exif("./Test Pictures/lake.jpg")
lake
animal= read_exif("./Test Pictures/animal1.jpg")
lake = read_exif("./Test Pictures/lake.jpg")
tiger = read_exif("./Test Pictures/tiger.jpg")
devtools::install_github("rstudio/rmarkdown")
rm(list = ls())
library(twitteR)
library(httr)
library(tm)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
# access Tokens
consumer_Key = "oQ3PqERg75kPtgBcgOLaFShSC"
consumer_Secret = "d4cxaKc1Dt3ugagruUNPtWzvmqGHx8WvwYAQ8MywUqTIVTTj9O"
access_Token = "833674399224061952-tL4gGOyGUrz84IbVlkkAmQzqUPahL1N"
access_Secret = "qkNmkD7TU5uZtIENW3r5K20wkqfbL6w37xyXLwelYBZg6"
##  Intially, the function asks the user to cache the credentials and
##  will be used for another session.
setup_twitter_oauth(consumer_Key,consumer_Secret,access_Token,access_Secret)
##  Creating a token manually
app <- oauth_app("twitter", key=consumer_Key, secret=consumer_Secret)
token = Token1.0$new(endpoint = NULL, params = list(as_header = TRUE),
app = app, credentials = list(oauth_token = access_Token,
oauth_token_secret = access_Secret))
saveRDS(token,"tokenTest")
test_Token = readRDS("tokenTest")
## token and test_Token are the same object.
## loading cached token from twitteR package:
##  It failed, likely because of the output format.
##  The file size is 0KB
# oauth_content <- readRDS('.httr-oauth')
?searchTwitter
TEST= searchTwitter(search_String, n=200,
lang=lang, since=since,geocode =geocode,query=list(tweet_mode="extended"))
##  Some parameters
search_String = "NRC+OR+#NRC+OR+@NRC"
lang = "en"
since = "2016-01-01"
##  Extracting coordinates for center of Canada:
google_Api_Key = "AIzaSyBGTs-gZCbyP8n0Hvw_VZ76Z6YrST1DNa8"
google_Host = "https://maps.googleapis.com/maps/api"
request = paste(google_Host,"/geocode/json",
"?address=Canada&key=",
google_Api_Key,sep="")
raw= GET(request)
data = jsonlite::fromJSON(
httr:: content(raw,as="text")
)
lat = data$results$geometry$location["lat"]
lng = data$results$geometry$location["lng"]
geocode =paste(lat,lng,"2000km",sep=",")
##  A list of tweets in Ottawa mentioning NRC. Note, the return
## already a "status"
NRC_Search = searchTwitter(search_String, n=200,
lang=lang, since=since,geocode =geocode)
##  built in functions allow the specification of "untruncated tweets"
##  This was done manually.
##  Many tweets are truncated. Getting a list
## of ids for tweets that have been truncated.
truncated_Id = lapply(NRC_Search, function(x)
{
if(x$truncated)
return(x$id)
else
return(NA)
}
)
version = 1.1
cmd = "/statuses/show/"
param = "?tweet_mode=extended"
search_Id = truncated_Id[
!is.na(truncated_Id)]
long_Tweet = list();
##  Getting Untruncated tweets
##  Going to use traditionall get methods
for(i in 1:length(search_Id))
{
url = paste("https://api.twitter.com/",
version,cmd,
search_Id[i],".json",
param, sep="")
##getting raw response
raw_Response = GET(url,config=token)
##  expanded
long_Tweet[[i]] = jsonlite::fromJSON(
httr::content(raw_Response,"text")
)
}
truncated_Id[!is.na(truncated_Id)] = long_Tweet
## Organized texts results
for(i in 1:length(NRC_Search)){
if(NRC_Search[[i]]$truncated){
NRC_Search[[i]] = truncated_Id[[i]]
}
}
## removing twitter links
text_NRC = lapply(NRC_Search, function(x)
{
text = x$text
## converting to ASCII
text= iconv(x=text,from="UTF-8",to="ASCII",sub="")
##Clearning out URLs
text=gsub("http(s?)://t.co/[a-zA-Z0-9]+",
"",text)
text=gsub("\nhttps:","",text)
}
)
rm(list = setdiff(ls(),c("token","text_NRC")))
##  Some parameters
search_String = "NRC+OR+#NRC+OR+@NRC"
lang = "en"
since = "2016-01-01"
##  Extracting coordinates for center of Canada:
google_Api_Key = "AIzaSyBGTs-gZCbyP8n0Hvw_VZ76Z6YrST1DNa8"
google_Host = "https://maps.googleapis.com/maps/api"
request = paste(google_Host,"/geocode/json",
"?address=Canada&key=",
google_Api_Key,sep="")
raw= GET(request)
data = jsonlite::fromJSON(
httr:: content(raw,as="text")
)
lat = data$results$geometry$location["lat"]
lng = data$results$geometry$location["lng"]
geocode =paste(lat,lng,"2000km",sep=",")
##  A list of tweets in Ottawa mentioning NRC. Note, the return
## already a "status"
NRC_Search = searchTwitter(search_String, n=200,
lang=lang, since=since,geocode =geocode)
##  built in functions allow the specification of "untruncated tweets"
##  This was done manually.
##  Many tweets are truncated. Getting a list
## of ids for tweets that have been truncated.
truncated_Id = lapply(NRC_Search, function(x)
{
if(x$truncated)
return(x$id)
else
return(NA)
}
)
version = 1.1
cmd = "/statuses/show/"
param = "?tweet_mode=extended"
search_Id = truncated_Id[
!is.na(truncated_Id)]
long_Tweet = list();
##  Getting Untruncated tweets
##  Going to use traditionall get methods
for(i in 1:length(search_Id))
{
url = paste("https://api.twitter.com/",
version,cmd,
search_Id[i],".json",
param, sep="")
##getting raw response
raw_Response = GET(url,config=token)
##  expanded
long_Tweet[[i]] = jsonlite::fromJSON(
httr::content(raw_Response,"text")
)
}
truncated_Id[!is.na(truncated_Id)] = long_Tweet
## Organized texts results
for(i in 1:length(NRC_Search)){
if(NRC_Search[[i]]$truncated){
NRC_Search[[i]] = truncated_Id[[i]]
}
}
## removing twitter links
text_NRC = lapply(NRC_Search, function(x)
{
text = x$text
## converting to ASCII
text= iconv(x=text,from="UTF-8",to="ASCII",sub="")
##Clearning out URLs
text=gsub("http(s?)://t.co/[a-zA-Z0-9]+",
"",text)
text=gsub("\nhttps:","",text)
}
)
# rm(list = setdiff(ls(),c("token","text_NRC")))
NRC_Search = searchTwitter(search_String, n=200,
lang=lang, since=since,geocode =geocode,query=list(tweet_mode="extended"))
NRC_Search = searchTwitter(search_String, n=200,
lang=lang, since=since,geocode =geocode,query=list("tweet_mode"="extended"))
require(twitteR)
##  Constructing corpus (structure to process text)
cor= Corpus(VectorSource(text_NRC))
cor = tm_map(cor,removePunctuation)
cor = tm_map(cor, removeWords,stopwords('english'))
cor = tm_map(cor,stemDocument)
## Some words may appear to be missing characters
## This is due to the stem analysis function.
wordcloud(cor,max.words = 100,random.color=T,random.order = T,
colors =brewer.pal(8,"Paired") )
##  screen names can be found by looking under the profile
## picture on twitter, denoted by the @ follwed by the screen name
screen_Name = c("NRC_API_Testing","CNN",
"DarrenCriss","CTVNews","MasterChef")
## object "users" automatically returned
users = lookupUsers(users= screen_Name)
# examining friendship data
friendships = lapply(users, function(x)
{
friendships(screen_names=x$screenName)
})
# One user
test_User = users[[2]]
friends = test_User$getFriends()
users
users$CNN
users$CNN$id
